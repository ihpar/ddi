{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install simplemma\n",
    "# !pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from simplemma import text_lemmatizer\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = None\n",
    "with open(\"stopwords.txt\", \"r\") as in_file:\n",
    "    stop_words = set(in_file.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = None\n",
    "with open(\"metin_1.txt\", \"r\") as in_file:\n",
    "    doc = in_file.read()\n",
    "\n",
    "baslik = doc.split(\"\\n\", 1)[0]\n",
    "metin = doc.split(\"\\n\", 1)[1]\n",
    "\n",
    "print(\"Başlık:\")\n",
    "print(baslik)\n",
    "\n",
    "print(\"*\" * 50)\n",
    "\n",
    "print(\"Metin:\")\n",
    "print(metin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumleler = sent_tokenize(metin)\n",
    "cumleler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(\"’|“|”|‘|–|—\", \" \", text)\n",
    "    text = text_lemmatizer(text, lang=\"tr\")\n",
    "    return \" \".join([word for word in text if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumleler_temiz = [clean_text(c) for c in cumleler]\n",
    "cumleler_temiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(norm=\"l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(cumleler_temiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = X[0]\n",
    "v1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1[v1 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1[v1 != 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumle_puani(vektor):\n",
    "    non_zero_elems = vektor[vektor != 0]\n",
    "    return non_zero_elems.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skorlar = np.array([cumle_puani(v) for v in X])\n",
    "skorlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siralama_indeksleri = np.argsort(-skorlar)\n",
    "siralama_indeksleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Başlık:\")\n",
    "print(baslik)\n",
    "print(\"Özet:\")\n",
    "for i in siralama_indeksleri[:5]:\n",
    "    print(cumleler[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metin_ozetle(dosya_ismi, cumle_sayisi=5):\n",
    "    doc = None\n",
    "    with open(dosya_ismi, \"r\") as in_file:\n",
    "        doc = in_file.read()\n",
    "\n",
    "    baslik = doc.split(\"\\n\", 1)[0]\n",
    "    metin = doc.split(\"\\n\", 1)[1]\n",
    "    cumleler = sent_tokenize(metin)\n",
    "    cumleler_temiz = [clean_text(c) for c in cumleler]\n",
    "    vectorizer = TfidfVectorizer(norm=\"l1\")\n",
    "    X = vectorizer.fit_transform(cumleler_temiz)\n",
    "    skorlar = np.array([cumle_puani(v) for v in X])\n",
    "    siralama_indeksleri = np.argsort(-skorlar)\n",
    "\n",
    "    print(baslik)\n",
    "    print(\"Özet:\")\n",
    "    for i in siralama_indeksleri[:cumle_sayisi]:\n",
    "        print(cumleler[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metin_ozetle(\"metin_2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hazır kullanım için metin özetleyiciler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim_3.8.3/auto_examples/tutorials/run_summarization.html\n",
    "\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ozetleyici = TextRankSummarizer()\n",
    "parser = PlaintextParser.from_string(metin, Tokenizer(\"tr\"))\n",
    "ozet = ozetleyici(parser.document, sentences_count=5)\n",
    "for cumle in ozet:\n",
    "    print(str(cumle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ozetleyici = LsaSummarizer()\n",
    "ozet = ozetleyici(parser.document, sentences_count=5)\n",
    "for cumle in ozet:\n",
    "    print(str(cumle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumy_ile_ozetle(dosya, method=\"text_rank\", cumle_sayisi=5):\n",
    "    doc = None\n",
    "    with open(dosya, \"r\") as in_file:\n",
    "        doc = in_file.read()\n",
    "\n",
    "    baslik = doc.split(\"\\n\", 1)[0]\n",
    "    metin = doc.split(\"\\n\", 1)[1]\n",
    "\n",
    "    parser = PlaintextParser.from_string(metin, Tokenizer(\"tr\"))\n",
    "\n",
    "    ozetleyici = None\n",
    "    if method == \"text_rank\":\n",
    "        ozetleyici = TextRankSummarizer()\n",
    "    if method == \"lsa\":\n",
    "        ozetleyici = LsaSummarizer()\n",
    "\n",
    "    ozet = ozetleyici(parser.document, sentences_count=cumle_sayisi)\n",
    "    print(baslik, \"\\n\")\n",
    "    for cumle in ozet:\n",
    "        print(str(cumle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumy_ile_ozetle(\"metin_2.txt\", method=\"text_rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumy_ile_ozetle(\"metin_2.txt\", method=\"lsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstractive Yöntem\n",
    "# https://github.com/alisafaya/mukayese/tree/main\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13507ca3de264013b157c2802cb6c998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/408 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d9337e9b6947518fc53f1741281752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c607ade3496409d8da53217aeb4a4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/8.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b990951ba91a4be7bcd8eaff99e6fea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2de2ff34c6c48f3b9e77d76d91feda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a89253efca442da816e0583922e2ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mukayese/mt5-base-turkish-sum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mukayese/mt5-base-turkish-sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"UEFA Şampiyonlar Ligi'nde Real Madrid'e olaylı şekilde boyun eğen Paris Saint Germain'de oyuncuların gruplaşmaya başladığı öne sürüldü.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = \"\"\"Fransız devi PSG'nin üzerindeki kara bulutlar dağılmıyor. \n",
    "Devler Ligi'nde Real Madrid'e olaylı şekilde boyun eğen başkent temsilcisinde oyuncuların gruplaşmaya başladığı öne sürüldü. \n",
    "Güney Amerikalılar ve Fransızca konuşanlar olarak ikiye ayrılan oyuncuların saha içerisinde de birbirlerine uzak olduğu iddia edildi. \n",
    "İşte PSG'de soyunma odasında yaşananlar ve 20 milyon avroluk tazminat ihtimali... \n",
    "UEFA Şampiyonlar Ligi'nde Real Madrid'e sansasyonel bir şekilde elenen Paris Saint Germain'de Kylian Mbappe haricindeki tüm oyunculara yönelik taraftar tepkisinin devam etmesi başkent temsilcisindeki krizi derinleştirdi.\n",
    "RMC Sport'ta yer alan haberde;\n",
    "Paris Saint Germain'in soyunma odasında işlerin yolunda gitmediği ve futbolcuların iki gruba ayrıldığı öne sürüldü. İddiaya göre oyuncular gruplaşmaya başladı ve aralarındaki iletişim her geçen gün zayıflıyor.\"\"\"\n",
    "\n",
    "inputs = tokenizer([article], max_length=1024, return_tensors=\"pt\")\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=6, max_length=100)\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
